\documentclass{article}
\usepackage[sexy, hdr, fancy]{evan}
\setlength{\droptitle}{-4em}

\lhead{Homework 6 Solutions}
\rhead{Linear Algebra and Differential Equations}
\lfoot{}
\cfoot{\thepage}

\begin{document}
\title{Homework 6 Solutions}
\maketitle
\thispagestyle{fancy}

\begin{enumerate}
	\item (E\&P 3.6.62) Prove this as follows. Given $x_1, x_2,$ and $x_3,$ define the cubic polynomial $P(y)$ to be
		\begin{align*}
			P(y) = \det\begin{bmatrix}
				1 & x_1 & x_1^2 & x_1^3 \\
				1 & x_2 & x_2^2 & x_2^3 \\
				1 & x_3 & x_3^2 & x_3^3 \\
				1 & y & y^2 & y^3
			\end{bmatrix} \tag{26}
		\end{align*}
		Because $P(x_1)=P(x_2)=P(x_3)=0$ (why?), the roots of $P(y)$ are $x_1, x_2,$ and $x_3.$ It follows that
		\begin{align*}
			P(y) &= k(y-x_1)(y-x_2)(y-x_3)
		\end{align*}
	where $k$ is the coefficient of $y^3$ in $P(y).$ Finally, observe that expansion of the $4\times 4$ determinant in (26) along its last row gives $k=V(x_1, x_2, x_3)$ and that $V(x_1, x_2, x_3, x_4) = P(x_4).$
	\begin{proof}
		First, note that
		\begin{align*}
			P(x_1) &= \det \begin{bmatrix}
				1 & x_1 & x_1^2 & x_1^3 \\
				1 & x_2 & x_2^2 & x_2^3 \\
				1 & x_3 & x_3^2 & x_3^3 \\
				1 & x_1& x_1^2 & x_1^3
			\end{bmatrix}
		\end{align*}
		Here, this matrix has two identical rows, so its determinant is 0, and thus $P(x_1)=0.$ Similarly, $P(x_2)=P(x_3)=0,$ so $x_1, x_2,$ and $x_3$ are the roots of $P,$ so it factors in the form given. Now, expanding the determinant along the 4th row, the coefficient of $y^3$ is
		\begin{align*}
			\det\begin{bmatrix}
				1 & x_1 & x_1^2 \\
				1 & x_2 & x_2^2 \\
				1 & x_3 & x_3^2
			\end{bmatrix} = V(x_1, x_2, x_3)
		\end{align*}
		as desired, and if we substitute $y=x_4,$ then $P(x_4)$ takes on the form of $V(x_1, x_2, x_3, x_4),$ as desired.
	\end{proof}

	\item (E\&P 4.7.26) Use the method of Example 8 to find a basis for the 2-dimensional solution space of the given differential equation. $y''+10y'=0.$
		\begin{soln}
			Let $v(x)=y'(x).$ Then this equation says $v' + 10v=0\implies v' = -10v,$ which has solution $v(x)=Ce^{-10x}.$ Thus,
			\begin{align*}
				y(x) &= \int y'(x)\, dx = \int v(x)\, dx = \int Ce^{-10x}\, dx = -\frac{1}{10} Ce^{-10x} + A
			\end{align*}
			so the general solution is given by $y(x)=A+Be^{-10x},$ so a basis of this solution space is $\left\{ 1, e^{-10x} \right\}.$
		\end{soln}

	\item (E\&P 4.7.32) Let $\bf A$ and $\bf B$ be $4\times 4$ (real) matrices partitioned into $2\times 2$ submatrices or "block":
		\begin{align*}
			\bf A = \begin{bmatrix}
				\bf A_{11} & \bf A_{12} \\ \bf A_{21} & \bf A_{22}
			\end{bmatrix}, \quad \bf B = \begin{bmatrix}
				\bf B_{11} & \bf B_{12} \\ \bf B_{21} & \bf B_{22}
			\end{bmatrix}
		\end{align*}
		Then verify that $\bf{AB}$ can be calculated in "blockwise" fashion:
		\begin{align*}
			\bf{AB} &= \begin{bmatrix}
				\bf{A_{11}B_{11} + A_{12}B_{21}} & \bf{A_{11}B_{12} + A_{12}B_{22}} \\
				\bf{A_{21}B_{11} + A_{22}B_{21}} & \bf{A_{21}B_{12} + A_{22}B_{22}}
			\end{bmatrix}
		\end{align*}
		\begin{soln}
			Let
			\begin{align*}
				\bf A &= \begin{bmatrix}
					a_{11} & a_{12} & a_{13} & a_{14} \\
					a_{21} & a_{22} & a_{23} & a_{24} \\
					a_{31} & a_{32} & a_{33} & a_{34} \\
					a_{41} & a_{42} & a_{43} & a_{44}
				\end{bmatrix}, \quad B = \begin{bmatrix}
					b_{11} & b_{12} & b_{13} & b_{14} \\
					b_{21} & b_{22} & b_{23} & b_{24} \\
					b_{31} & b_{32} & b_{33} & b_{34} \\
					b_{41} & b_{42} & b_{43} & b_{44}
				\end{bmatrix}
			\end{align*}
			Consider only the top left block of $\bf{AB},$ since every other quadrant is computed similarly. We have
			\begin{align*}
				\bf A_{11} &= \begin{bmatrix}
					a_{11} & a_{12} \\ a_{21} & a_{22}
				\end{bmatrix}, \quad\bf B_{11} =\begin{bmatrix}
					b_{11} & b_{12} \\ b_{21} & b_{22}
				\end{bmatrix} \\
				\bf A_{12} &= \begin{bmatrix}
					a_{13} & a_{14} \\
					a_{23} & a_{24}
				\end{bmatrix}, \quad\bf B_{21} = \begin{bmatrix}
					b_{31} & b_{32} \\ b_{41} & b_{42}
				\end{bmatrix} \\
				\implies \bf{A_{11}B_{11} + A_{12} B_{21} } &= \begin{bmatrix}
					a_{11} & a_{12} \\ a_{21} & a_{22}
				\end{bmatrix}\begin{bmatrix}
					b_{11} & b_{12} \\ b_{21} & b_{22}
				\end{bmatrix} + \begin{bmatrix}
					a_{13} & a_{14} \\
					a_{23} & a_{24}
				\end{bmatrix}\begin{bmatrix}
					b_{31} & b_{32} \\ b_{41} & b_{42}
				\end{bmatrix} \\
				&= \begin{bmatrix}
					a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\
					a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}
				\end{bmatrix} + \begin{bmatrix}
					a_{13}b_{31} + a_{14}b_{41} & a_{13}b_{32} + a_{14}b_{42} \\
					a_{23}b_{31} + a_{24}b_{41} & a_{23}b_{32} + a_{24}b_{42}
				\end{bmatrix}
			\end{align*}
			which is the same as the upper left $2\times 2$ block we obtain if we multiply $\bf{AB}$ directly.
		\end{soln}

	\item Determine if $y_1=x^3-1, y_2=x+1, y_3=x^3+x^2,$ and $y_4=x^2+x$ form a basis for $\mathcal P_4$ (the space of all polynomials of degree less than 4.)
		\begin{soln}
			We claim this is a basis for $\mathcal P_4.$ We must show these span $\mathcal P_4$ and are linearly independent. Since $\mathcal P_4$ is a vector space of dimension 4, these are equivalent, so we only need to show one of them.

			To show they are spanning, take $ax^3+bx^2+cx+d\in\mathcal P_4$ to be an arbitrary element of $\mathcal P_4.$ Now, let $k_1, k_2, k_3, k_4\in\RR$ such that
			\begin{align*}
				ax^3+bx^2+cx+d &= k_1(x^3-1) + k_2(x+1) + k_3(x^3+x^2) + k_4(x^2+x) \\
				&= (k_1+k_3)x^3 + (k_3+k_4)x^2 + (k_2+k_4)x + (k_2-k_1)
			\end{align*}
			so by matching coefficients, we have the 4 equations
			\begin{align*}
				a &= k_1+k_3 \\
				b &= k_3+k_4 \\
				c &= k_2+k_4 \\
				d &= k_2-k_1 \\
				\implies \begin{bmatrix}
					a \\ b \\ c \\ d
				\end{bmatrix} &= \begin{bmatrix}
					1 & 0 & 1 & 0 \\
					0 & 0 & 1 & 1 \\
					0 & 1 & 0 & 1 \\
					-1 & 1 & 0 & 0
				\end{bmatrix}\begin{bmatrix}
					k_1 \\ k_2 \\ k_3 \\ k_4
				\end{bmatrix}
			\end{align*}
			Now, by performing Gaussian elimination on this matrix, we find that it is non-singular, and thus a solution exists for $k_1, k_2, k_3, k_4,$ so any arbitrary element of $\mathcal P_4$ can be represented as a linear combination of the 4 given polynomials, so $y_1, y_2, y_3, y_4$ span $\mathcal P_4,$ and thus form a basis.

			Alternatively, we can show that the polynomials are linearly independent. Suppose
			\begin{align*}
				0 &= k_1y_1+k_2y_2+k_3y_3+k_4y_4 \\
				\implies \begin{bmatrix}
					0 \\ 0 \\ 0 \\ 0
				\end{bmatrix} &= \begin{bmatrix}
					1 & 0 & 1 & 0 \\
					0 & 0 & 1 & 1 \\
					0 & 1 & 0 & 1 \\
					-1 & 1 & 0 & 0
				\end{bmatrix}\begin{bmatrix}
					k_1 \\ k_2 \\ k_3 \\ k_4
				\end{bmatrix}
			\end{align*}
			By performing Gaussian elimination, we find that $k_1=k_2=k_3=k_4,$ so it follows that $y_1, y_2, y_3, y_4$ are linearly independent, and thus they form a basis for $\mathcal P_4.$
		\end{soln}

	\item MATLAB Practice Lesson 10

	\item MATLAB Practice Lesson 11

	\item Let $\bf A$ be an $m\times n$ matrix with $m>n.$ Consider the linear system $\bf{Ax} = \bf b.$ This system is usually inconsistent so instead, we attempt to find a vector $\hat{\bf x}$ such that $\left\lVert \bf{A\hat{x}}-\bf{b} \right\rVert_2$ is as small as possible. The vector $\hat{\bf x}$ is called the least squares solution for the system.
		\begin{itemize}
				\ii We may find $\hat{\bf {x}}$ by solving 
				\begin{align*}
					\bf{A}^T \bf{A\hat{x}} = \bf{A}^T\bf{b}
				\end{align*}

				\ii The orthogonal projection of $\bf b$ onto the column space of $\bf A$ is given by
				\begin{align*}
					\hat{\bf b} = \bf{A\hat x}
				\end{align*}
		\end{itemize}
		Let
		\begin{align*}
			\bf A = \begin{bmatrix}
				1 & 1 \\ -1 & 1 \\ -1 & 2
			\end{bmatrix} \quad \bf b = \begin{bmatrix}
				5 \\ 4 \\ 5
			\end{bmatrix}
		\end{align*}
		\begin{enumerate}[(a)]
			\item Find the least squares solution for the system $\bf{Ax}=\bf b.$
				\begin{soln}
					We have
					\begin{align*}
						\bf{A^T A\hat x} &= \bf{A^T b}\implies \bf{\hat x} = (\bf{A^T A})\inv \bf{A^T b} 
					\end{align*}
					so the least squares solution can be solved as
					\begin{align*}
						\bf{A^T A} &= \begin{bmatrix}
							1 & -1 & -1 \\ 1 & 1 & 2
						\end{bmatrix} \begin{bmatrix}
							1 & 1 \\ -1 & 1 \\ -1 & 2
						\end{bmatrix} = \begin{bmatrix}
							3 & -2 \\ -2 & 6
						\end{bmatrix} \\
						\implies (\bf{A^T A})\inv &= \frac{1}{3\cdot 6 - (-2)\cdot (-2)} \begin{bmatrix}
							6 & 2 \\ 2 & 3
						\end{bmatrix} = \frac{1}{14} \begin{bmatrix}
							6 & 2 \\ 2 & 3
						\end{bmatrix} \\
						\implies \bf{\hat x} &= \frac{1}{14}\begin{bmatrix}
							6 & 2 \\ 2 & 3
						\end{bmatrix}\begin{bmatrix}
							1 & -1 & -1 \\ 1 & 1 & 2
						\end{bmatrix}\begin{bmatrix}
							5 \\ 4 \\ 5
						\end{bmatrix} = \frac{1}{14}\begin{bmatrix}
							14 \\ 49
						\end{bmatrix} = \begin{bmatrix}
							1 \\ 7/2
						\end{bmatrix}
					\end{align*}
				\end{soln}

			\item Find the orthogonal projection of $\hat b$ onto the column space of $\bf A.$
				\begin{soln}
					We have
					\begin{align*}
						\hat{\bf b} &= \bf{A\hat x} = \begin{bmatrix}
							1 & 1 \\ -1 & 1 \\ -1 & 2
						\end{bmatrix}\begin{bmatrix}
							1 \\ 7/2
						\end{bmatrix} = \begin{bmatrix}
							9/2 \\ 5/2 \\ 6
						\end{bmatrix}
					\end{align*}
				\end{soln}

		\end{enumerate}

	\item (E\&P 5.1.23) Determine whether the pair of functions are linearly independent or linearly dependent on the real line. $f(x)=xe^x, y(x)=\abs{x}e^x.$
		\begin{soln}
			These functions are linearly dependent if they are constant multiples on the real line. Suppose
			\begin{align*}
				\frac{f(x)}{g(x)} &= k \implies \frac{xe^x}{\abs{x}e^x} = \frac{x}{\abs{x}}=k
			\end{align*}
			Now, if $x\ge 0,$ then $\abs{x}=x,$ but if $x<0$ then $\abs{x}=-x,$ so it is clear that there is no value of $k$ that satisfies this equation, and thus $f$ and $g$ are linearly independent.
		\end{soln}

\end{enumerate}

\end{document}
