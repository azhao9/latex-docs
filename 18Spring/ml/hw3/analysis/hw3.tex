\documentclass{article}
\usepackage[sexy, hdr, fancy]{evan}
\setlength{\droptitle}{-4em}

\lhead{Homework 3}
\rhead{Machine Learning}
\lfoot{}
\cfoot{\thepage}
\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\begin{document}
\title{Homework 3}
\maketitle
\thispagestyle{fancy}


\section{Analytical (50 points)}

\paragraph{1) Deep Neural Networks (12 points)}

\begin{enumerate}[(a)]
	\item Consider a 2-layer neural network, with $M$ input nodes, $Z$ nodes in the hidden layer and $K$ nodes in the output layer. The network is fully connected, i.e. every node in the $n-1$th layer is connected to every node in the $n$th layer. However, for your application of interest, you suspect that only some of the nodes in the input are relevant. How would you modify the objective function to reflect this belief?

	\item Consider a $N$ layer neural network. We could (a) train the entire network at once using back-propagation or (b) pre-train each layer individually, and then tune the final network with back-propagation. Will (a) and (b) converge to the same solution? Why would we favor strategy (a) vs. strategy (b)?

	\item Consider a $N\ge2$ layer neural network with a single node in the output layer. We wish to train this network for binary classification. Rather than use a cross entropy objective, we want to take a max-margin approach and ensure a margin of $\gamma=1$. Describe the structure of the last layer of the network, including the final activation function, and the training objective function that implements a max-margin neural network. What are the benefits of this network compared to one trained with cross entropy? Will a max-margin trained neural network learn the same decision boundary as an SVM?


\end{enumerate}


\paragraph{2) Adaboost (12 points)} There is one good example at $x=0$ and two negative examples at $x = \pm 1$. There are three weak classifiers are
\begin{align*}
	h_1(x) & = 1\cdot{\bf1}(x > 1/2) -1\cdot{\bf1}(x \leq 1/2),\\
	h_2(x) & = 1\cdot{\bf1}(x > -1/2) -1\cdot{\bf1}(x \leq -1/2)\\
	h_3(x) & =1.
\end{align*}
Show that this data can be classified correctly by a strong classifier which uses only three weak classifiers. Calculate the first two iterations of AdaBoost for this problem. Are they sufficient to classify the data correctly?
\begin{soln}
	We have
	\begin{align*}
		\mathcal D = \left\{ (0, 1), (-1, -1), (1, -1) \right\}
	\end{align*}
	We wish to find constants $\alpha_1, \alpha_2, \alpha_3$ such that
	\begin{align*}
		h(x_i) = \alpha_1h_1(x_i) + \alpha_2h_2(x_i) + \alpha_3h_3(x_i) = y_i
	\end{align*}
	for our three training examples. Evaluating $h_1, h_2, h_3$ at each of $x_1, x_2, x_3,$ we have the equations
	\begin{align*}
		-\alpha_1 + \alpha_2 + \alpha_3 &= 1 \\
		-\alpha_1 - \alpha_2 + \alpha_3 &= -1 \\
		\alpha_1+\alpha_2+\alpha_3 &= -1
	\end{align*}
	which has solution $(\alpha_1, \alpha_2, \alpha_3) = (-1, 1, -1),$ so the data is correctly classified by the strong classifier 
	\begin{align*}
		h(x) = -h_1(x) + h_2(x) - h_3(x)
	\end{align*}

	Applying the Adaboost algorithm, we initialize $D_1 = \begin{bmatrix}
		1/3 & 1/3 & 1/3
	\end{bmatrix}.$ Now, using $h_1$ as the first hypothesis, we have
	\begin{align*}
		h_1(x_1) &= -1 \neq y_1 \\
		h_1(x_2) &= -1 = y_2 \\
		h_1(x_3) &= 1\neq y_3
	\end{align*}
	so
	\begin{align*}
		\varepsilon_1 &= \frac{2}{3} \implies \alpha_1 = \frac{1}{2}\log \frac{1/3}{2/3} = -\frac{1}{2}\log 2 
	\end{align*}
	Now, we update the distribution as
	\begin{align*}
		Z_2 &= \sum_{i=1}^{3} D_1(i) \exp(-\alpha_1 y_i h_1(\mathbf{x_i})) = \frac{1}{3} \exp\left( -\frac{1}{2}\log 2 \right) + \frac{1}{3}\exp\left( \frac{1}{2}\log 2 \right) + \frac{1}{3}\left( -\frac{1}{2}\log 2 \right) \\
		&= \frac{1}{3} \left( \frac{1}{\sqrt{2}} + \sqrt{2} + \frac{1}{\sqrt{2}} \right) = \frac{2\sqrt{2}}{3} \\
		D_2(1) &= \frac{D_1(1)}{Z_2} \exp\left( -\alpha_1y_1h_1(\mathbf{x_1}) \right) = \frac{1/3}{2\sqrt{2}/3} \exp\left( -\frac{1}{2}\log 2 \right) = \frac{1}{2\sqrt{2}}\cdot \frac{1}{\sqrt{2}} =\frac{1}{4} \\
		D_2(2) &= \frac{D_1(2)}{Z_2}\exp\left( -\alpha_1y_2h_1(\mathbf{x_2}) \right) = \frac{1/3}{2\sqrt{2/3}}\exp\left( \frac{1}{2}\log 2 \right) = \frac{1}{2\sqrt{2}}\cdot \sqrt{2} = \frac{1}{2} \\
		D_2(3) &= \frac{1}{4}
	\end{align*}

	Now, using $h_2$ as the second hypothesis, we have
	\begin{align*}
		h_2(x_1) &= 1 = y_1 \\
		h_2(x_2) &= 1 \neq y_2 \\
		h_2(x_3) &= 1 \neq y_3
	\end{align*}
	so
	\begin{align*}
		\varepsilon_2 &= \frac{1}{2} + \frac{1}{4} = \frac{3}{4} \implies \alpha_2 = \frac{1}{2} \log \frac{1/4}{3/4} = -\frac{1}{2}\log 3
	\end{align*}
	Now, we update the distribution as
	\begin{align*}
		Z_3 &= \sum_{i=1}^{3} D_2(i)\exp\left( -\alpha_2 y_i h_2(x_i) \right) = \frac{1}{4}\exp\left( \frac{1}{2}\log 3 \right) + \frac{1}{2}\exp\left( -\frac{1}{2}\log 3 \right) + \frac{1}{4}\exp\left( -\frac{1}{2}\log 3 \right) \\
		&= \frac{1}{4} \sqrt{3} + \frac{1}{2\sqrt{3}} + \frac{1}{4\sqrt{3}} = \frac{\sqrt{3}}{2} \\
		D_3(1) &= \frac{D_2(1)}{Z_3}\exp\left( -\alpha_2 y_1 h_2(x_1) \right) = \frac{1/4}{\sqrt{3}/2}\exp\left( \frac{1}{2}\log 3 \right) = \frac{1}{2\sqrt{3}}\cdot\sqrt{3} = \frac{1}{2} \\
		D_3(2) &= \frac{D_2(2)}{Z_3}\exp\left( -\alpha_2 y_2h_2(x_2) \right) = \frac{1/2}{\sqrt{3}/2}\exp\left( -\frac{1}{2}\log 3 \right) = \frac{1}{\sqrt{3}}\cdot \frac{1}{\sqrt{3}} = \frac{1}{3} \\
		D_3(3) &= \frac{1}{6}
	\end{align*}
	so our final model is
	\begin{align*}
		h(x) = \alpha_1h_1(x) + \alpha_2h_2(x) = -\frac{1}{2} = \left( -\frac{1}{2}\log 2 \right) h_1(x) + \left( -\frac{1}{2}\log 3 \right) h_2(x)
	\end{align*}<++>
\end{soln}<++>

\paragraph{3) Ensemble Methods (12 points)}

Consider the following binary classification Boosting algorithm.
\begin{enumerate}
	\item Given $\{\vxi, \yi\}_{i=1}^N$, number of iterations $T$, weak learner $f$.
	\item Initialize $\D_0$ to be a uniform distribution over examples.
	\item For each iteration $t = 1 \ldots T$:
		\begin{enumerate}
			\item Train a weak learner $f$ on the data given $\D_t$ to produce hypothesis $h_t$.
			\item Compute the error of $h_t$ as $\epsilon_t = P_{\D_t} [h_t(\vxi) \ne \yi]$
			\item Compute $\alpha_t = \frac{1}{2} \log \frac{1-\epsilon_t}{\epsilon_t}$
			\item Update $\D$ as:\\
				$\D_{t+1}(i) = \frac{\D_t(i)}{Z_t} \times \left\{
					\begin{array}{lr}
						\exp(-\alpha_t + (T-t) / T) ~~  \textrm{if} ~~ h_t(\vxi) = \yi  \\
						\exp(\alpha_t + (T-t) / T) ~~ \textrm{otherwise}
					\end{array}
					\right.$
		\end{enumerate}
	\item Output final hypothesis $H(\vx)=\textrm{sign} \left\{ \sum_{t=1}^T \alpha_t h_t(\vx) \right\}$
	\end{enumerate}

	$Z_t$ is a normalization constant so that $\D$ is a valid probability distribution.

	Describe the difference between this algorithm and the AdaBoost algorithm we learned about in class. What problem of AdaBoost is this change designed to fix? How does changing the algorithm's user provided parameter affect this behavior?
	\begin{soln}
		In this algorithm.
	\end{soln}<++>

	\paragraph{4. Overfitting in Clustering (14 points)}

	Given the data set $x_1,...,x_n$, we want cluster the data using the K-means algorithm. The K-means algorithm aims to partition the $n$ observations into $k$ sets ($k < n$) $S = \{S_1, S_2, \ldots, S_k\}$ so as to minimize the within-cluster sum of squares
	\begin{eqnarray}
		\mathop{\textrm{argmin}}_{S=\{S_1,...,S_k\}}\sum_{j=1}^k\sum_{x_i\in S_j}\|x_j-\mu_j\|_2^2
		\label{objective1}
	\end{eqnarray}
	where $\mu_j$ is the mean of points in $S_j$.

	\begin{enumerate}[(a)]
		\item Let $\gamma_k$ denote the optimal value of the objective function, prove $\gamma_k$ is non-increasing in $k$.
			\begin{proof}
				Define
				\begin{align*}
					f(S) = \sum_{S_j\in S}^{}\sum_{x_i\in S_j}^{}\left\lVert x_j-\mu_j \right\rVert_2^2
				\end{align*}
				to be the objective function evaluated on a partition $S.$ Suppose $S=\left\{ S_1, \cdots, S_k \right\}$ is the partition that gives the optimal objective function value for $k.$ Suppose we created a $k+1$th cluster centered at any of the data points, WLOG $x_1\in S_1.$ Then for the partition $T=\left\{ S_1\setminus\left\{ x_1 \right\}, S_2, \cdots, S_k, \left\{ x_1 \right\} \right\},$ we have
				\begin{align*}
					f(T) &= \sum_{T_j\in T}^{}\sum_{x_i\in T_j}^{}\left\lVert x_j-\mu_j \right\rVert_2^2 = \sum_{S_j\in S}^{}\sum_{x_i\in S_j}^{}\left\lVert x_j-\mu_j \right\rVert_2^2 - \left\lVert x_1-\mu_1 \right\rVert_2^2 + \left\lVert x_1-x_1 \right\rVert_2^2 \\
					&= \gamma_k - \left\lVert x_1-\mu_1 \right\rVert_2^2 \\
					&\le \gamma_k
				\end{align*}
				Since $T$ is a partition with $k+1$ sets, and $\gamma_{k+1}$ is the minimum objective function value over all partition with $k+1$ sets, it follows that
				\begin{align*}
					\gamma_{k+1} \le f(T) \le f(S) = \gamma_{k}
				\end{align*}
				and thus $\gamma_k$ is non-increasing on $k.$
			\end{proof}
		\item Suppose we modified the objective function as follows:
			\begin{eqnarray}
				\mathop{\textrm{argmin}}_{S=\{S_1,...,S_k\}}\sum_{j=1}^k\sum_{x_i\in S_j}\max(\|x_j-\mu_j\|_2^2, \tau)
				\label{objective2}
			\end{eqnarray}
			where $\tau$ is some (given) constant and $\gamma'_k$ is the optimal value of this new objective function. Compare the values of 
			$\gamma_k$ and $\gamma'_k$ ($<, \le, =, \ge, >$) and prove this relation.
		\item K-medoids is an algorithm similar to K-means. Both K-means and K-medoids attempt to minimize the squared error but unlike K-means, K-medoids chooses a provided example as a cluster center (medoids) rather than the mean of a subset of the examples. For a given data set $\X$, compare the optimal clusterings produced by K-means and K-medoids ($<, \le, =, \ge, >$) and prove this relation.
		\item Suppose you wanted to select $k$ (the number of clusters) to minimize the objective function. Should you work with objective \ref{objective1} or \ref{objective2}? If \ref{objective2}, how does your choice of $\tau$ effect your choice of $k$?
	\end{enumerate}


	\end{document}
