\documentclass{article}
\usepackage[sexy, hdr, fancy]{evan}
\setlength{\droptitle}{-4em}

\lhead{Homework 3}
\rhead{Machine Learning}
\lfoot{}
\cfoot{\thepage}
\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\begin{document}
\title{Homework 3}
\maketitle
\thispagestyle{fancy}


\section{Analytical (50 points)}

\paragraph{1) Deep Neural Networks (12 points)}

\begin{enumerate}[(a)]
	\item Consider a 2-layer neural network, with $M$ input nodes, $Z$ nodes in the hidden layer and $K$ nodes in the output layer. The network is fully connected, i.e. every node in the $n-1$th layer is connected to every node in the $n$th layer. However, for your application of interest, you suspect that only some of the nodes in the input are relevant. How would you modify the objective function to reflect this belief?

	\item Consider a $N$ layer neural network. We could (a) train the entire network at once using back-propagation or (b) pre-train each layer individually, and then tune the final network with back-propagation. Will (a) and (b) converge to the same solution? Why would we favor strategy (a) vs. strategy (b)?

	\item Consider a $N\ge2$ layer neural network with a single node in the output layer. We wish to train this network for binary classification. Rather than use a cross entropy objective, we want to take a max-margin approach and ensure a margin of $\gamma=1$. Describe the structure of the last layer of the network, including the final activation function, and the training objective function that implements a max-margin neural network. What are the benefits of this network compared to one trained with cross entropy? Will a max-margin trained neural network learn the same decision boundary as an SVM?


\end{enumerate}


\paragraph{2) Adaboost (12 points)} There is one good example at $x=0$ and two negative examples at $x = \pm 1$. There are three weak classifiers are
\begin{align*}
	h_1(x) & = 1\cdot{\bf1}(x > 1/2) -1\cdot{\bf1}(x \leq 1/2),\\
	h_2(x) & = 1\cdot{\bf1}(x > -1/2) -1\cdot{\bf1}(x \leq -1/2)\\
	h_3(x) & =1.
\end{align*}
Show that this data can be classified correctly by a strong classifier which uses only three weak classifiers. Calculate the first two iterations of AdaBoost for this problem. Are they sufficient to classify the data correctly?
\begin{soln}
	We have
	\begin{align*}
		\mathcal D = \left\{ (0, 1), (-1, -1), (1, -1) \right\}
	\end{align*}
	We wish to find constants $\alpha_1, \alpha_2, \alpha_3$ such that
	\begin{align*}
		h(x_i) = \alpha_1h_1(x_i) + \alpha_2h_2(x_i) + \alpha_3h_3(x_i) = y_i
	\end{align*}
	for our three training examples. Evaluating $h_1, h_2, h_3$ at each of $x_1, x_2, x_3,$ we have the equations
	\begin{align*}
		-\alpha_1 + \alpha_2 + \alpha_3 &= 1 \\
		-\alpha_1 - \alpha_2 + \alpha_3 &= -1 \\
		\alpha_1+\alpha_2+\alpha_3 &= -1
	\end{align*}
	which has solution $(\alpha_1, \alpha_2, \alpha_3) = (-1, 1, -1),$ so the data is correctly classified by the strong classifier 
	\begin{align*}
		h(x) = -h_1(x) + h_2(x) - h_3(x)
	\end{align*}

	Applying the Adaboost algorithm, we initialize $D_1 = \begin{bmatrix}
		1/3 & 1/3 & 1/3
	\end{bmatrix}.$ Now, using $h_1$ as the first hypothesis, we have
	\begin{align*}
		h_1(x_1) &= -1 \neq 1 = y_1 \\
		h_1(x_2) &= -1 = -1 = y_2 \\
		h_1(x_3) &= 1\neq -1 = y_3
	\end{align*}
	so
	\begin{align*}
		\varepsilon_1 &= \frac{2}{3} \implies \alpha_1 = \frac{1}{2}\log \frac{1/3}{2/3} = -\frac{1}{2}\log 2 
	\end{align*}
	Now, we update the distribution as
	\begin{align*}
		Z_2 &= \sum_{i=1}^{3} D_1(i) \exp(-\alpha_1 y_i h_1(\mathbf{x_i})) = \frac{1}{3} \exp\left( -\frac{1}{2}\log 2 \right) + \frac{1}{3}\exp\left( \frac{1}{2}\log 2 \right) + \frac{1}{3}\left( -\frac{1}{2}\log 2 \right) \\
		&= \frac{1}{3} \left( \frac{1}{\sqrt{2}} + \sqrt{2} + \frac{1}{\sqrt{2}} \right) = \frac{2\sqrt{2}}{3} \\
		D_2(1) &= \frac{D_1(1)}{Z_2} \exp\left( -\alpha_1y_1h_1(\mathbf{x_1}) \right) = \frac{1/3}{2\sqrt{2}/3} \exp\left( -\frac{1}{2}\log 2 \right) = \frac{1}{2\sqrt{2}}\cdot \frac{1}{\sqrt{2}} =\frac{1}{4} \\
		D_2(2) &= \frac{D_1(2)}{Z_2}\exp\left( -\alpha_1y_2h_1(\mathbf{x_2}) \right) = \frac{1/3}{2\sqrt{2/3}}\exp\left( \frac{1}{2}\log 2 \right) = \frac{1}{2\sqrt{2}}\cdot \sqrt{2} = \frac{1}{2} \\
		D_2(3) &= \frac{1}{4}
	\end{align*}

	Now, using $h_2$ as the second hypothesis, we have
	\begin{align*}
		h_2(x_1) &= 1 = 1 = y_1 \\
		h_2(x_2) &= -1 =-1 = y_2 \\
		h_2(x_3) &= 1 \neq y_3
	\end{align*}
	so
	\begin{align*}
		\varepsilon_2 &= \frac{1}{4}  \implies \alpha_2 = \frac{1}{2} \log \frac{3/4}{1/4} = \frac{1}{2}\log 3
	\end{align*}
	Now, we update the distribution as
	\begin{align*}
		Z_3 &= \sum_{i=1}^{3} D_2(i)\exp\left( -\alpha_2 y_i h_2(x_i) \right) = \frac{1}{4}\exp\left( -\frac{1}{2}\log 3 \right) + \frac{1}{2}\exp\left( -\frac{1}{2}\log 3 \right) + \frac{1}{4}\exp\left( \frac{1}{2}\log 3 \right) \\
		&= \frac{1}{4} \frac{1}{\sqrt{3}} + \frac{1}{2}\frac{1}{\sqrt{3}} + \frac{1}{4}\sqrt{3}= \frac{\sqrt{3}}{2} \\
		D_3(1) &= \frac{D_2(1)}{Z_3}\exp\left( -\alpha_2 y_1 h_2(x_1) \right) = \frac{1/4}{\sqrt{3}/2}\exp\left(- \frac{1}{2}\log 3 \right) = \frac{1}{2\sqrt{3}}\cdot\frac{1}{\sqrt{3}} = \frac{1}{6} \\
		D_3(2) &= \frac{D_2(2)}{Z_3}\exp\left( -\alpha_2 y_2h_2(x_2) \right) = \frac{1/2}{\sqrt{3}/2}\exp\left( -\frac{1}{2}\log 3 \right) = \frac{1}{\sqrt{3}}\cdot \frac{1}{\sqrt{3}} = \frac{1}{3} \\
		D_3(3) &= \frac{1}{2}
	\end{align*}
	so our final model is
	\begin{align*}
		H(x) &= \sign\{\alpha_1h_1(x) + \alpha_2h_2(x)\} = \sign\left\{\left( -\frac{1}{2}\log 2 \right) h_1(x) + \left( \frac{1}{2}\log 3 \right) h_2(x) \right\}
	\end{align*}

	This model is insufficient to classify our data. Consider the point $(1, -1).$ We have
	\begin{align*}
		h_1(1) &= 1, \quad h_2(1) = 1 \\
		\implies H(1) &= \sign\left\{ -\frac{1}{2}\log 2 + \frac{1}{2}\log 3 \right\} = 1 \neq -1
	\end{align*}
\end{soln}

\paragraph{3) Ensemble Methods (12 points)}

Consider the following binary classification Boosting algorithm.
\begin{enumerate}
	\item Given $\{\vxi, \yi\}_{i=1}^N$, number of iterations $T$, weak learner $f$.
	\item Initialize $\D_0$ to be a uniform distribution over examples.
	\item For each iteration $t = 1 \ldots T$:
		\begin{enumerate}
			\item Train a weak learner $f$ on the data given $\D_t$ to produce hypothesis $h_t$.
			\item Compute the error of $h_t$ as $\epsilon_t = P_{\D_t} [h_t(\vxi) \ne \yi]$
			\item Compute $\alpha_t = \frac{1}{2} \log \frac{1-\epsilon_t}{\epsilon_t}$
			\item Update $\D$ as:\\
				$\D_{t+1}(i) = \frac{\D_t(i)}{Z_t} \times \left\{
					\begin{array}{lr}
						\exp(-\alpha_t + (T-t) / T) ~~  \textrm{if} ~~ h_t(\vxi) = \yi  \\
						\exp(\alpha_t + (T-t) / T) ~~ \textrm{otherwise}
					\end{array}
					\right.$
		\end{enumerate}
	\item Output final hypothesis $H(\vx)=\textrm{sign} \left\{ \sum_{t=1}^T \alpha_t h_t(\vx) \right\}$
	\end{enumerate}

	$Z_t$ is a normalization constant so that $\D$ is a valid probability distribution.

	Describe the difference between this algorithm and the AdaBoost algorithm we learned about in class. What problem of AdaBoost is this change designed to fix? How does changing the algorithm's user provided parameter affect this behavior?
	\begin{soln}
		As written, this algorithm behaves identically to the original AdaBoost algorithm. Here, we have
		\begin{align*}
			D_{t+1}(i) &= \frac{D_t(i)}{Z_t} \exp\left\{ -\alpha_t y_i h_t(x_i) \right\} \exp\left\{ (T-t)/T \right\}
		\end{align*}
		where $Z_t$ is the normalizing constant
		\begin{align*}
			Z_t &= \sum_{i=1}^{N} D_t(i) \exp\left\{ -\alpha_t y_i h_t(x_i) \right\} \exp\left\{ (T-t)/T \right\} = \exp\left\{ (T-t)/T \right\} \sum_{i=1}^{N} D_t(i)\exp\left\{ -\alpha_t y_i h_t(x_i) \right\} \\
			\implies D_{t+1}(i) &= \frac{D_t(i)}{\exp\left\{ (T-t)/T \right\}\sum_{i=1}^{N} D_t(i)\exp\left\{ -\alpha_t y_i h_t(x_i) \right\}} \exp\left\{ -\alpha_t y_i h_t(x_i) \right\} \exp\left\{ (T-t)/T \right\} \\
			&= \frac{D_t(i)}{\sum_{i=1}^{N} \exp\left\{ -\alpha_t y_i h_t(x_i) \right\}}\exp\left\{ -\alpha_t y_i h_t(x_i) \right\}
		\end{align*}
		which is exactly the update performed in the original AdaBoost algorithm. Larger values of $T$ will increase the number of hypotheses used, which is expected to improve the test accuracy of AdaBoost since it is resistant to overfitting.
	\end{soln}

	\paragraph{4. Overfitting in Clustering (14 points)}

	Given the data set $x_1,...,x_n$, we want cluster the data using the K-means algorithm. The K-means algorithm aims to partition the $n$ observations into $k$ sets ($k < n$) $S = \{S_1, S_2, \ldots, S_k\}$ so as to minimize the within-cluster sum of squares
	\begin{eqnarray}
		\mathop{\textrm{argmin}}_{S=\{S_1,...,S_k\}}\sum_{j=1}^k\sum_{x_i\in S_j}\|x_j-\mu_j\|_2^2
		\label{objective1}
	\end{eqnarray}
	where $\mu_j$ is the mean of points in $S_j$.

	\begin{enumerate}[(a)]
		\item Let $\gamma_k$ denote the optimal value of the objective function, prove $\gamma_k$ is non-increasing in $k$.
			\begin{proof}
				Define
				\begin{align*}
					f(S) = \sum_{S_j\in S}^{}\sum_{x_i\in S_j}^{}\left\lVert x_j-\mu_j \right\rVert_2^2
				\end{align*}
				to be the objective function evaluated on a partition $S.$ Suppose $S=\left\{ S_1, \cdots, S_k \right\}$ is the partition that gives the optimal objective function value for $k.$ Suppose we created a $k+1$th cluster centered at any of the data points, WLOG $x_1\in S_1.$ Then for the partition $T=\left\{ S_1\setminus\left\{ x_1 \right\}, S_2, \cdots, S_k, \left\{ x_1 \right\} \right\},$ we have
				\begin{align*}
					f(T) &= \sum_{T_j\in T}^{}\sum_{x_i\in T_j}^{}\left\lVert x_j-\mu_j \right\rVert_2^2 = \sum_{S_j\in S}^{}\sum_{x_i\in S_j}^{}\left\lVert x_j-\mu_j \right\rVert_2^2 - \left\lVert x_1-\mu_1 \right\rVert_2^2 + \left\lVert x_1-x_1 \right\rVert_2^2 \\
					&= \gamma_k - \left\lVert x_1-\mu_1 \right\rVert_2^2 \\
					&\le \gamma_k
				\end{align*}
				Since $T$ is a partition with $k+1$ sets, and $\gamma_{k+1}$ is the minimum objective function value over all partition with $k+1$ sets, it follows that
				\begin{align*}
					\gamma_{k+1} \le f(T) \le f(S) = \gamma_{k}
				\end{align*}
				and thus $\gamma_k$ is non-increasing on $k.$
			\end{proof}

		\item Suppose we modified the objective function as follows:
			\begin{eqnarray}
				\mathop{\textrm{argmin}}_{S=\{S_1,...,S_k\}}\sum_{j=1}^k\sum_{x_i\in S_j}\max(\|x_j-\mu_j\|_2^2, \tau)
				\label{objective2}
			\end{eqnarray}
			where $\tau$ is some (given) constant and $\gamma'_k$ is the optimal value of this new objective function. Compare the values of 
			$\gamma_k$ and $\gamma'_k$ ($<, \le, =, \ge, >$) and prove this relation.
			\begin{proof}
				We claim that $\gamma_k \le \gamma'_k.$ Let $S=\left\{ S_1, \cdots, S_k \right\}$ be the optimal partition for $\gamma'_k,$ and let $f(S)$ be the objective function for $\gamma_k,$ and let $g(S, \tau)$ be the objective function for $\gamma'_k,$ both evaluated on the partition $S.$ For any point $x_i,$ we have
				\begin{align*}
					\left\lVert x_i-\mu_j \right\rVert_2^2 &\le \max\left\{ \left\lVert x_i-\mu_j \right\rVert_2^2, \tau \right\} \\
					\implies f(S) = \sum_{j=1}^{k} \sum_{x_i\in S_j}^{} \left\lVert x_i-\mu_j \right\rVert_2^2 &\le \sum_{j=1}^{k} \sum_{x_i\in S_j}^{} \max\left\{ \left\lVert x_i-\mu_j \right\rVert_2^2, \tau \right\} = g(S, \tau) = \gamma'_k
				\end{align*}
				Thus, since $\gamma_k\le f(S),$ we conclude that $\gamma_k\le \gamma'_k.$

				We can have equality if $\tau \le \left\lVert x_i-\mu_j \right\rVert_2^2$ for all $i, j.$ However, the inequality does not necessarily hold in the other direction. Suppose $\gamma_k>\gamma'_k$ for some $k$ and let $S=\left\{ S_1, \cdots, S_k \right\}$ be the optimal partition for $\gamma_k$ and let $T=\left\{ T_1, \cdots, T_k \right\}$ be the optimal partition for $\gamma'_k.$ Then 
				\begin{align*}
					f(S) &= \sum_{j=1}^{k} \sum_{x_i\in S_j}^{}\left\lVert x_i-\mu(S_j) \right\rVert_2^2 > \sum_{j=1}^{k} \sum_{x_i\in T_j}^{}\max\left\{ \left\lVert x_i-\mu(T_j) \right\rVert_2^2, \tau \right\} = g(T, \tau)
				\end{align*}
				for a given $\tau.$ Then we have
				\begin{align*}
					\sum_{j=1}^{k} \sum_{x_i\in T_j}^{}\max\left\{ \left\lVert x_i-\mu(T_j) \right\rVert_2^2, \tau \right\} \ge \sum_{j=1}^{k} \sum_{x_i\in T_j}^{} \left\lVert x_i-\mu(T_j) \right\rVert_2^2 = f(T)
				\end{align*}
				so it follows that $f(T)<f(S),$ which is a contradiction since we assumed that $f(S)$ was minimal. Thus, we have the final relation $\gamma_k\le \gamma'_k.$
			\end{proof}

		\item K-medoids is an algorithm similar to K-means. Both K-means and K-medoids attempt to minimize the squared error but unlike K-means, K-medoids chooses a provided example as a cluster center (medoids) rather than the mean of a subset of the examples. For a given data set $\X$, compare the optimal clusterings produced by K-means and K-medoids ($<, \le, =, \ge, >$) and prove this relation.
			\begin{proof}
				Let $\varphi_k$ be the optimal objective function value for K-medoids on $k$ clusters. We claim that $\gamma_k\le \varphi_k.$ Let $f(S)$ be the objective function value for K-means evaluated on the partition $S,$ and let $g(S, M)$ be the objective function values for K-medoids evaluated on the partition $S$ with set of median points $M.$ Fix $k,$ and let $S=\left\{ S_1, \cdots, S_k \right\}, M=\left\{ m_1, \cdots, m_k \right\}$ be the optimal clustering and choice of median for K-medoids, so that $\varphi_k=g(S, M).$
				
				Consider cluster $j.$ Then if $E(\alpha) = \sum_{x_i\in S_j}^{} \left\lVert x_i-\alpha \right\rVert_2^2,$ it is well known that $\alpha=\frac{1}{\abs{S_j}}\sum_{x_i\in S_j}^{}x_i = \mu_j$ minimizes $E(\alpha),$ the squared error. Thus,
				\begin{align*}
					\sum_{x_i\in S_j}^{}\left\lVert x_i-\mu_j \right\rVert_2^2&\le \sum_{x_i\in S_j}^{}\left\lVert x_i-m_j \right\rVert_2^2 \\
					\implies f(S) = \sum_{j=1}^{k} \sum_{x_i\in S_j}^{}\left\lVert x_i-\mu_j \right\rVert_2^2 &\le \sum_{j=1}^{k} \sum_{x_i\in S_j}^{}\left\lVert x_i-m_j \right\rVert_2^2 = g(S, M) = \varphi_k
				\end{align*}
				Thus, since $\gamma_k\le f(S),$ we conclude that $\gamma_k\le \varphi_k.$

				We can have equality if we take $k=N$ so that every data point is its own cluster. In this case, the mean and median of every cluster is the same, and the objective function values are both 0. However, the inequality does not necessarily hold in the other direction. Suppose $\gamma_k>\varphi_k$ for some $k$ and let $S=\left\{ S_1, \cdots, S_k \right\}$ be the optimal partition for K-means, and let $T=\left\{ T_1, \cdots, T_k \right\}, M=\left\{ m_1, \cdots, m_k \right\}$ be the optimal partition and choice of medians for K-medoids. Then
				\begin{align*}
					f(S) = \sum_{j=1}^{k} \sum_{x_i\in S_j}^{} \left\lVert x_i-\mu_j \right\rVert_2^2 &> \sum_{j=1}^{k} \sum_{x_i\in T_j}^{}\left\lVert x_i-m_j \right\rVert_2^2 = g(T, M)
				\end{align*}
				Then from above, we have
				\begin{align*}
					\sum_{j=1}^{k} \sum_{x_i\in T_j}^{}\left\lVert x_i-m_j \right\rVert_2^2 \ge \sum_{j=1}^{k} \sum_{x_i\in T_j}^{}\left\lVert x_i - \mu(T_j) \right\rVert_2^2 = f(T)
				\end{align*}
				so it follows that $f(T) < f(S),$ which is a contradiction since we assumed that $f(S)$ was minimal. Thus, we have the final relation $\gamma_k\le \varphi_k.$
			\end{proof}

		\item Suppose you wanted to select $k$ (the number of clusters) to minimize the objective function. Should you work with objective \ref{objective1} or \ref{objective2}? If \ref{objective2}, how does your choice of $\tau$ effect your choice of $k$?
			\begin{soln}
				WLOG, all data points are unique. If we use objective 1, to achieve the minimum objective function we must have $k=N,$ which will give an objective function value of 0.

				If we use objective 2, assuming $\tau$ is a positive constant, then taking $k=N$ again we achieve the minimum objective function value of $N\tau.$ WLOG the distance between $x_1$ and $x_2$ is minimal in the data set, so that $\left\lVert x_1-x_2 \right\rVert_2^2 \le \left\lVert x_i-x_j \right\rVert_2^2$ for all $i\neq j.$ Take $\tau\ge\frac{1}{4}\left\lVert x_1-x_2 \right\rVert_2^2.$ Then if $S=\left\{ \left\{ x_1, x_2 \right\}, x_3, \cdots, x_N \right\},$ we have
				\begin{align*}
					g(S, \tau) &= \sum_{j=1}^{N-1} \sum_{x_i\in S_j}^{}\max\left\{ \left\lVert x_i-\mu_j \right\rVert_2^2, \tau \right\} \\
					&= \max\left\{ \left\lVert x_1-\frac{x_1+x_2}{2} \right\rVert_2^2, \tau \right\} + \max\left\{ \left\lVert x_2-\frac{x_1+x_2}{2} \right\rVert_2^2, \tau \right\} + (N-2)\tau \\
					&= \max\left\{ \left\lVert \frac{x_1-x_2}{2} \right\rVert_2^2, \tau \right\} + \max\left\{ \left\lVert \frac{x_2-x_1}{2} \right\rVert_2^2, \tau \right\} + (N-2)\tau \\
					&= 2\max\left\{ \frac{1}{4}\left\lVert x_1-x_2 \right\rVert_2^2, \tau \right\} + (N-2)\tau = 2\tau + (N-2)\tau = N\tau
				\end{align*}
				Thus, the minimum objective function value can be achieved using $k-1$ clusters. Generalizing, if we increase $\tau,$ we can achieve the minimum objective function value using smaller $k.$
			\end{soln}
	\end{enumerate}


	\end{document}
