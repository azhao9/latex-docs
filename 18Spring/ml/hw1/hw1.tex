\documentclass{article}
\usepackage[sexy, hdr, fancy]{evan}
\setlength{\droptitle}{-4em}

\lhead{Homework 1}
\rhead{Machine Learning}
\lfoot{}
\cfoot{\thepage}

\begin{document}
\title{Homework 1}
\maketitle
\thispagestyle{fancy}

\begin{enumerate}[1)]
	\item \textbf{Supervised vs. Unsupervised Learning}
		\begin{enumerate}[1.]
			\item Give an example of a problem that could be solved with both supervised and unsupervised learning. Is data readily available for this problem? How would you measure your 'success' in solving this problem for each approach?

			\item What are the pros and cons of each approach? Which approach do you think the problem better lends itself to?
				
		\end{enumerate}

	\item \textbf{Model Complexity} Explain when you would want to use a simple model over a complex model and vice versa. Are there any approaches you could use to mitigate the disadvantages of using a complex model?

	\item \textbf{Training and Generalization} Suppose you're building a system to classify images of food into two categories: either the image contains a hot dog or it does not. You're given a dataset of 25,000 (image, label) pairs for training and a separate dataset of 5,000 (image, label) pairs.
		\begin{enumerate}[1.]
			\item Suppose you train an algorithm and obtain 96\% accuracy on the larger training set. Do you expect the trained model to obtain similar performance if used on newly acquired data? Why or why not?

			\item Suppose that, after training, someone gives you a new test set of 1,000 (image, label) pairs. Which do you expect to give greater accuracy on the test set: The model after trained on the dataset of 25,000 pairs or the model after trained on the dataset of 5,000 pairs? Explain your reasoning.

			\item Suppose your models obtained greater than 90\% accuracy on the test set. How might you proceed in hope of improving accuracy further?

		\end{enumerate}

	\item \textbf{Loss Function} State whether each of the following is a valid loss function for binary classification. Wherever a loss function is not valid, state why. Here, $y$ is the correct label and $\hat y$ is a decision confidence value, meaning that the predicted value is given by $\sign(\hat y)$ and the confidence on the classification increases with $\abs{\hat y}.$
		\begin{enumerate}[1.]
			\item $\ell(y, \hat y)=\frac{3}{4}\left( y-\hat y \right)^2$
				\begin{answer*}
					This is valid.
				\end{answer*}

			\item $\ell(y, \hat y)=\abs{(y-\hat y)}/\hat y$
				\begin{answer*}
					This is not valid. The loss can be arbitrarily large if $\hat y_i=-c$ for $c>0.$
				\end{answer*}
				
			\item $\ell(y, \hat y)=\max(0, 1-y\cdot \hat y).$
				
		\end{enumerate}

	\item \textbf{Linear Regression} Suppose you observe $n$ data points $(x_1, y_2), \cdots, (x_n, y_n),$ where all $x_i$ and all $y_i$ are scalars. 
		\begin{enumerate}[1.]
			\item Suppose you choose the model $\hat y=wx$ and aim to minimize the sum of squares error $\sum_{i}^{}(y_i-\hat y_i)^2.$ Derive the closed form solution for $w$ from scratch, where 'from scratch' means without using the least-squares solution presented in class.
				\begin{soln}
					The sum of squares error is given by
					\begin{align*}
						\sum_{i=1}^{n} (y_i-\hat y_i)^2 &= \sum_{i=1}^{n} (y_i- wx_i)^2 = \sum_{i=1}^{n}(y_i^2-2wx_iy_i+w^2x_i^2) \\
						&= \sum_{i=1}^{n} y_i^2 - 2w\sum_{i=1}^{n} x_iy_i + w^2\sum_{i=1}^{n} x_i^2
					\end{align*}
					To minimize this, take the partial with respect to $w$ and setting equal to 0,
					\begin{align*}
						0&=\frac{\partial}{\partial w} \left( \sum_{i=1}^{n} y_i^2 - 2w\sum_{i=1}^{n} x_iy_i + w^2\sum_{i=1}^{n} x_i^2 \right) = -2\sum_{i=1}^{n} x_iy_i + 2w\sum_{i=1}^{n} x_i^2 \\
						\implies w &= \frac{\sum_{i=1}^{}x_iy_i}{\sum_{i=1}^{} x_i^2}
					\end{align*}
				\end{soln}

			\item Suppose you instead choose the model $\hat y=w\sin x$ and aim to minimize the sum of squares error $\sum_{i}^{}(y_i-\hat y_i)^2.$ Is there a closed-form solution for $w?$ If so, what is it?
				\begin{soln}
					The sum of squares error is given by
					\begin{align*}
						\sum_{i=1}^{n} (y_i-\hat y_i)^2 &= \sum_{i=1}^{n} (y_i-w\sin x_i)^2 = \sum_{i=1}^{n} (y_i^2-2wy_i\sin x_i+w^2\sin^2x_i) \\
						&= \sum_{i=1}^{n} y_i^2 - 2w\sum_{i=1}^{n} y_i\sin x_i + w^2\sum_{i=1}^{n} \sin^2x_i
					\end{align*}
					To minimize this, take the partial with respect to $w$ and setting equal to 0,
					\begin{align*}
						0 &= \frac{\partial}{\partial w} \left( \sum_{i=1}^{n} y_i^2 - 2w\sum_{i=1}^{n} y_i\sin x_i  + w^2\sum_{i=1}^{n} \sin^2x_i\right) = -2\sum_{i=1}^{n} y_i\sin x_i + 2w\sum_{i=1}^{n} \sin^2 x_i \\
						\implies w &= \frac{\sum_{i}^{}y_i\sin x_i}{\sum_{i}^{}\sin^2x_i}
					\end{align*}
				\end{soln}
				
		\end{enumerate}

	\item \textbf{Logistic Regression} Explain whether each statement is true or false. If false, explain why.
		\begin{enumerate}[1.]
			\item In the case of binary classification, optimizing the logistic loss is equivalent to minimizing the sum-of-squares error between our predicted probabilities for class 1, $\hat{\bf y},$ and the observed probabilities for class 1, $\bf y.$

			\item One possible advantage of stochastic gradient descent is that it can sometimes escape local minima. However, in the case of logistic regression, the global minimum is the only minimum, and stochastic gradient descent is therefore never useful.
				\begin{answer*}
					This is false. The fact that the global minimum is the only minimum means that stochastic gradient descent will always converge to the correct minimum.
				\end{answer*}
				
		\end{enumerate}
		
\end{enumerate}

\end{document}
