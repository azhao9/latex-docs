\documentclass{article}
\usepackage[sexy, hdr, fancy]{evan}
\setlength{\droptitle}{-4em}

\lhead{Homework 6}
\rhead{Investment Science}
\lfoot{}
\cfoot{\thepage}

\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}

\begin{document}
\title{Homework 6}
\maketitle
\thispagestyle{fancy}

\begin{enumerate}
	\item 
		\begin{soln}
			If $X$ and $Y$ are the results of the two dice, then $Z=XY.$ The rolls are independent of each other, and we have
			\begin{align*}
				E[X] = E[Y] &= \frac{1}{6}(1+2+3+4+5+6) = \frac{7}{2} \\
				E[X^2] = E[Y^2] &= \frac{1}{6}(1^2+2^2+3^2+4^2+5^2+6^2) = \frac{91}{6}
			\end{align*}
			Now, since $X$ and $Y$ are independent, it also holds that $X^2$ and $Y^2$ are independent, so
			\begin{align*}
				E[Z] &= E[XY] = E[X]E[Y] = \frac{7}{2}\cdot \frac{7}{2} = \frac{49}{4} \\
				\var(Z) &= E[Z^2] - (E[Z])^2 = E[X^2Y^2] - (E[XY])^2 = E[X^2]E[Y^2]-(E[XY])^2 \\
				&= \frac{91}{6}\cdot \frac{91}{6} - \left( \frac{49}{4} \right)^2 = \frac{11515}{144}
			\end{align*}
		\end{soln}

	\item 
		\begin{enumerate}[(a)]
			\item 
				\begin{soln}
					We wish to find $\alpha$ that minimizes 
					\begin{align*}
						L &= \frac{1}{2}\var[\alpha r_A + (1-\alpha) r_B] = \frac{1}{2}\left(\var(\alpha r_A) + \var[(1-\alpha)r_B] + 2\cov(\alpha r_A, (1-\alpha) r_B)\right) \\
						&= \frac{1}{2}\alpha^2 \sigma_A^2 + \frac{1}{2}(1-\alpha)^2\sigma_B^2 + \alpha(1-\alpha) \rho\sigma_A\sigma_B
					\end{align*}
					Taking the derivative with respect to $\alpha,$ we have
					\begin{align*}
						\frac{\partial L}{\partial \alpha} &= \alpha\sigma_A^2 - (1-\alpha)\sigma_B^2 + (1-2\alpha)\rho\sigma_A\sigma_B = 0 \\
						\implies \alpha &= \frac{\sigma_B^2-\rho\sigma_A\sigma_B}{\sigma_A^2+\sigma_B^2-2\rho\sigma_A\sigma_B} = 0.8261 \\
						\implies 1-\alpha &= 0.1739
					\end{align*}	
				\end{soln}

			\item 
				\begin{soln}
					Evaluating at $\alpha,$ we have
					\begin{align*}
						\sigma &= \sqrt{2L(\alpha)} = 1.94\%
					\end{align*}
				\end{soln}

			\item 
				\begin{soln}
					The expected return is
					\begin{align*}
						E[r] &= E[\alpha r_A + (1-\alpha)r_B] = \alpha \bar r_a + (1-\alpha)\bar r_B = 11.39\%
					\end{align*}
				\end{soln}

		\end{enumerate}

	\item 
		\begin{soln}
			If $\alpha$ is the weight of asset 1 and $1-\alpha$ is the weight of asset 2, then from above, we have
			\begin{align*}
				\alpha = \frac{\sigma_2^2 - \rho\sigma_1\sigma_2}{\sigma_1^2+\sigma_2^2-2\rho\sigma_1\sigma_2} = \frac{\sigma_2^2 - \sigma_{12}}{\sigma_1^2+\sigma_2^2-2\sigma_{12}}
			\end{align*}
			minimizes the variance of the portfolio. The expected return is
			\begin{align*}
				E[r] &= E[\alpha r_1+(1-\alpha) r_2] = \alpha \bar r_1 + (1-\alpha)\bar r_2 \\
				&= \bar r_1 + \frac{\sigma_2^2-\sigma_{12}}{\sigma_1^2+\sigma_2^2-2\sigma_{12}}(\bar r_1-\bar r_2)
			\end{align*}
		\end{soln}

	\item
		\begin{enumerate}[(a)]
			\item 
				\begin{soln}
					Each asset has the same return $\bar r_0,$ so if the portfolio was entirely a single asset, all of these points would lie on the horizontal line $\bar r = \bar r_0.$ Since they are uncorrelated, this line is exactly the minimum variance set, and the efficient frontier is the entire line.
				\end{soln}

			\item 
				\begin{soln}
					We have the Lagrangian
					\begin{align*}
						L &= \frac{1}{2} \sum_{i=1}^{n} w_i^2\sigma_i^2 - \mu\left( \sum_{i=1}^{n} w_i - 1 \right)
					\end{align*}
					Taking the partial derivatives with each of the $w_i,$ the weights with minimum variance satisfy
					\begin{align*}
						\frac{\partial L}{\partial w_i} &= w_i\sigma_i^2 - \mu = 0 \implies w_i = \frac{\mu}{\sigma_i^2} \\
						\implies \sum_{i=1}^{n} w_i &= \sum_{i=1}^{n} \frac{\mu}{\sigma_i^2} = \frac{\mu}{\bar\sigma^2} = 1 \implies \mu = \bar\sigma^2 \\
						\implies w_i &= \frac{\bar\sigma^2}{\sigma_i^2}
					\end{align*}
				\end{soln}

		\end{enumerate}

	\item 
		\begin{enumerate}[(a)]
			\item
				\begin{soln}
					If $w$ is the column vector of weights, we seek to minimize
					\begin{align*}
						\frac{1}{2}w^T V w &= \frac{1}{2}\begin{bmatrix}
							w_1 & w_2 & w_3
						\end{bmatrix}\begin{bmatrix}
							2 & 1 & 0 \\
							1 & 2 & 1 \\
							0 & 1 & 2
						\end{bmatrix}
						\begin{bmatrix}
							w_1 \\ w_2 \\ w_3
						\end{bmatrix} = \frac{1}{2}\begin{bmatrix}
							2w_1+w_2 & w_1+2w_2+w_3 & w_2+2w_3
						\end{bmatrix} \begin{bmatrix}
							w_1 \\ w_2 \\ w_3
						\end{bmatrix} \\
						&=\frac{1}{2}\left[ (2w_1^2+w_1w_2)+(w_1w_2+2w_2^2+w_2w_3)+(w_2w_3+2w_3^2)\right] \\
						&= w_1^2+w_2^2+w_3^2 + w_1w_2 + w_2w_3
					\end{align*}
					The Lagrangian and its derivatives are given by
					\begin{align*}
						L &= \frac{1}{2}w^T Vw - \mu\left( w_1+w_2+w_3-1 \right) \\
						\frac{\partial L}{\partial w_1} &= 2w_1+w_2-\mu = 0 \\
						\frac{\partial L}{\partial w_2} &= 2w_2 + w_1+w_3 - \mu = 0 \\
						\frac{\partial L}{\partial w_3} &= 2w_3+w_2 - \mu = 0 \\
						\frac{\partial L}{\partial\mu} &= -w_1-w_2-w_3+1 = 0
					\end{align*}
					and solving this system gives $w_1=0.5, w_2=0, w_3=0.5.$
				\end{soln}

			\item 
				\begin{soln}
					Setting $\lambda=1, \mu=0,$ we are not as concerned with the weight restriction so we have
					\begin{align*}
						2v_1 + v_2 &= 0.4 \\
						v_1+2v_2+v_3 &= 0.8 \\
						v_2 + 2v_3 &= 0.8 \\
						\implies\begin{bmatrix}
							v_1 \\ v_2 \\ v_3
						\end{bmatrix} &= \begin{bmatrix}
							0.1 \\ 0.2 \\ 0.3
						\end{bmatrix} \\
						\implies \begin{bmatrix}
							w_1 \\ w_2 \\ w_3
						\end{bmatrix} &= \begin{bmatrix}
							1/6 \\ 1/3 \\ 1/2
						\end{bmatrix}
					\end{align*}
					after normalizing.
				\end{soln}

			\item
				\begin{soln}
					If the risk free rate $r_f=0.2,$ then the equations become
					\begin{align*}
						2v_1+v_@ &= 0.4-0.2=0.2 \\
						v_1+2v_2+v_3 &= 0.8-0.2=0.6 \\
						v_2+2v_3 &= 0.8-0.2 = 0.6 \\
						\implies \begin{bmatrix}
							v_1 \\ v_2 \\ v_3
						\end{bmatrix} &= \begin{bmatrix}
							0 \\ 0.2 \\ 0.2
						\end{bmatrix} \\
						\implies \begin{bmatrix}
							w_1 \\ w_2 \\ w_3
						\end{bmatrix} &= \begin{bmatrix}
							0 \\ 1/2 \\ 1/2
						\end{bmatrix}
					\end{align*}
					after normalizing.
				\end{soln}

		\end{enumerate}

	\item 
		\begin{enumerate}[(a)]
			\item 
				\begin{soln}
					If $\sigma_{iM}$ is the covariance between $r_i$ and $r_M$ and $\sigma_M^2$ is the variance of $r_M,$ we have
					\begin{align*}
						\var(r-r_M) &= \var(r)+\var(r_M) - 2\cov(r, r_M) \\
						&= \var\left( \sum_{i=1}^{n} \alpha_i r_i \right) + \sigma_M^2 - 2\cov\left( \sum_{i=1}^{n} \alpha_i r_i, r_M \right) \\
						&= \sum_{i, j}^{}\alpha_i\alpha_j \sigma_{ij} + \sigma_M^2 - 2\sum_{i=1}^{n} \alpha_i \sigma_{iM}
					\end{align*}
					Now, we have the Lagrangian
					\begin{align*}
						L &= \frac{1}{2} \sum_{i, j}^{}\alpha_i\alpha_j \sigma_{ij} + \frac{1}{2} \sigma_M^2 - \sum_{i=1}^{n} \alpha_i \sigma_{iM} - \lambda\left( \sum_{i=1}^{n} \alpha_i - 1 \right)
					\end{align*}
					and its partial derivatives set equal to 0:
					\begin{align*}
						\frac{\partial L}{\partial \alpha_i} &= \sum_{j=1}^{n} \alpha_j \sigma_{ij} - \sigma_{iM} - \lambda = 0, \quad\forall i=1, 2, \cdots, n \\
						\frac{\partial L}{\partial \lambda} &= 1-\sum_{i=1}^{n} \alpha_i = 0
					\end{align*}
					are $n+1$ equations in $n+1$ variables $\alpha_1, \cdots, \alpha_n, \lambda.$
				\end{soln}

			\item
				\begin{soln}
					If we also wish to achieve a mean return of $\bar r_M,$ we have the additional constraint $\sum_{i=1}^{n} \alpha_i \bar r_i = \bar r_M,$ so the Lagrangian and its derivatives are
					\begin{align*}
						L &= \frac{1}{2} \sum_{i, j}^{}\alpha_i \alpha_j \sigma_{ij} + \frac{1}{2}\sigma_M^2 - \sum_{i=1}^{n} \alpha_i \sigma_{iM} - \lambda\left( \sum_{i=1}^{n} \alpha_i \bar r_i - \bar r_M \right) - \mu\left( \sum_{i=1}^{n} \alpha_i - 1 \right) \\
						\frac{\partial L}{\partial \alpha_i} &= \sum_{j=1}^{n} \alpha_j \sigma_{ij} - \sigma_{iM} - \lambda \bar r_i - \mu = 0, \quad\forall i=1, 2, \cdots, n \\
						\frac{\partial L}{\partial \lambda} &= \bar r_M - \sum_{i=1}^{n} \alpha_i \bar r_i = 0 \\
						\frac{\partial L}{\partial \mu} &= 1-\sum_{i=1}^{n} \alpha_i = 0
					\end{align*}
					which are $n+2$ equations in $n+2$ variables.
				\end{soln}
				
		\end{enumerate}

	\item 
		\begin{soln}
			We have
			\begin{align*}
				\frac{\partial}{\partial w_k}\left( \sum_{i, j}^{n} \sigma_{ij}w_i w_j \right)^{1/2} = \left( \sum_{i, j}^{n} \sigma_{ij}w_iw_j \right)^{-1/2} \sum_{i=1}^{n} \sigma_{ik} w_i
			\end{align*}
			Taking the derivative of $\tan \theta$ with respect to $w_k,$ we have
			\begin{align*}
				\frac{\partial}{\partial w_k} \tan \theta &= \frac{\partial}{\partial w_k} \left[\frac{\sum_{i=1}^{n} w_i(\bar r_i-r_f)}{\left( \sum_{i, j}^{}\sigma_{ij}w_iw_j \right)^{1/2}}\right] \\
				&= \frac{\left( \sum_{i, j}^{}\sigma_{ij}w_iw_j \right)^{1/2}\cdot \frac{\partial}{\partial w_k}\left[ \sum_{i=1}^{n} w_i(\bar r_i-r_f) \right] - \sum_{i=1}^{n} w_i(\bar r_i-r_f) \cdot \frac{\partial}{\partial w_k}\left[ \sum_{i, j}^{}\sigma_{ij}w_iw_j \right]^{1/2}}{\sum_{i, j}^{}\sigma_{ij}w_iw_j} \\
				&= \frac{\left(\sum_{i, j}^{}\sigma_{ij}w_iw_j\right)^{1/2}(\bar r_k-r_f) - \sum_{i=1}^{n} w_i(\bar r_i-r_f) \left(\sum_{i, j}^{}\sigma_{ij}w_iw_j\right)^{-1/2} \sum_{i=1}^{n} \sigma_{ik}w_i}{\sum_{i, j}^{}\sigma_{ij}w_iw_j} = 0 \\
				\implies \bar r_k -r_f &= \sum_{i=1}^{n} \sigma_{ik}w_i \sum_{i=1}^{n} w_i(\bar r_i-r_f)\left( \sum_{i, j}^{}\sigma_{ij}w_iw_j \right)\inv = \sum_{i=1}^{n} \sigma_{ik} \lambda w_i
			\end{align*}
			where
			\begin{align*}
				\lambda &= \sum_{i=1}^{n} w_i(\bar r_i-r_f) \left( \sum_{i, j}^{}\sigma_{ij}w_iw_j \right)\inv
			\end{align*}
			as desired.
		\end{soln}
\end{enumerate}

\end{document}
